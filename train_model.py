# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ie3pvmkm6YDNoNl3Fd23GwcQF-IuN3lh
"""

#1
!python3 -m pip install --upgrade "pip<24.1"

#2
!pip install ratsnlp

#21
!pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0

#3
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

# train/dev 나누기
# ① Colab에 파일 업로드
from google.colab import files
uploaded = files.upload()  # 여기서 data_q_a.json 선택

# ② JSON 열기
import json
from sklearn.model_selection import train_test_split

with open("data_q_a.json", "r", encoding="utf-8") as f:
    full_data = json.load(f)

# ③ train/val 나누기
train_data, val_data = train_test_split(full_data["data"], test_size=0.1, random_state=42)

# ④ Google Drive 경로에 저장
save_dir = "/gdrive/My Drive/nlpbook/korquad-v1"
!mkdir -p "{save_dir}"

with open(f"{save_dir}/KorQuAD_v1.0_train.json", "w", encoding="utf-8") as f:
    json.dump({"version": full_data.get("version", "1.0"), "data": train_data}, f, ensure_ascii=False, indent=2)

with open(f"{save_dir}/KorQuAD_v1.0_dev.json", "w", encoding="utf-8") as f:
    json.dump({"version": full_data.get("version", "1.0"), "data": val_data}, f, ensure_ascii=False, indent=2)

#4
import torch
from ratsnlp.nlpbook.qa import QATrainArguments
args = QATrainArguments(
    pretrained_model_name="beomi/kcbert-base",
    downstream_corpus_name="korquad-v1",
    downstream_corpus_root_dir="/gdrive/My Drive/nlpbook",
    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-qa",
    max_seq_length=128,
    max_query_length=32,
    doc_stride=64,
    batch_size=32 if torch.cuda.is_available() else 4,
    learning_rate=5e-5,
    epochs=3,
    tpu_cores=0 if torch.cuda.is_available() else 8,
    seed=7,
)

#5
from ratsnlp import nlpbook
nlpbook.set_seed(args)

#6
nlpbook.set_logger(args)

#7: 데이터를 직접 업로드하였으므로 주석처리
# nlpbook.download_downstream_dataset(args)

#8
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=False,
)

#9
from ratsnlp.nlpbook.qa import KorQuADV1Corpus, QADataset
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler
corpus = KorQuADV1Corpus()
train_dataset = QADataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="train",
)
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=RandomSampler(train_dataset, replacement=False),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)

#10
val_dataset = QADataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="val",
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=args.batch_size,
    sampler=SequentialSampler(val_dataset),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)

#11
from transformers import BertConfig
pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
)

#12
from transformers import BertForQuestionAnswering
model = BertForQuestionAnswering.from_pretrained(
        args.pretrained_model_name,
        config=pretrained_model_config,
)

#13
from ratsnlp.nlpbook.qa import QATask
task = QATask(model, args)

#14
trainer = nlpbook.get_trainer(args)

#15
trainer.fit(
    task,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader,
)

